# -*- coding: utf-8 -*-
"""Wuzzuf_Web_Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEovViUcJiD6hgrwmNBM3n1mFtTDo0hc

# **Wuzzauf Web Scraping Project**
This project demonstrates my approach to web scraping job listings from Wuzzuf using Python. The goal is to extract valuable job-related data — such as titles, companies, locations, and more — in a structured format.<br>

made by: <br>
*Mostafa Mohamed Hussein* <br>
*R1024#286*<br>

**Requisites** <br>
- Follow the same steps as the "machine learning" jobs scraping<br>
- Add (Company Location)<br>
- Scrap all the pages of search results (not the first page only)<br>
- Repeat for : "data analysis" , "data science" , "business intelligence"<br>
- Collect all in ( one CSV File )<br>

**Steps**<br>
- install new libraries ✔️ <br>
- import all libraries ✔️ <br>
- get all the required responses ✔️ <br>
- extract all the contents "parsing" ✔️ <br>
- fetch all needed informations ✔️ <br>
- organize all the collected data ✔️ <br>
- save the result in the proper format (in this case "CSV") ✔️ <br>
"""

#!pip install requests
#!pip install bs4

import requests
import pandas as pd
from bs4 import BeautifulSoup

class webscrap:

  def __init__(self):
    self.scraped_data = {
            'Title': [],
            'Link': [],
            'Occupation': [],
            'Company': [],
            'Specs': [],
            'Location': []
        }

  def fetch(self,url):
    self.url = url
    response = requests.get(self.url)
    response
    if response.status_code == 200:
      self.html = response.content
      self.soup = BeautifulSoup(self.html, 'lxml')
    else:
      print('Failed to fetch the page. please try again with another url')

  def get_titles(self,head,clas):
    self.head= head
    self.clas = clas
    titles = self.soup.find_all (self.head,{'class':self.clas})
    titles_lst = [title.text for title in titles]
    self.scraped_data['Title'] += titles_lst

  def get_links(self,head,clas):
    self.head= head
    self.clas = clas
    links = self.soup.find_all (self.head,{'class':self.clas})
    links_lst=[link.a['href'] for link in links]
    self.scraped_data['Link'] += links_lst

  def get_occupations(self,head,clas):
    self.head= head
    self.clas = clas
    occupations = self.soup.find_all(self.head,{'class':self.clas})
    occupations_lst = [occupation.text for occupation in occupations]
    self.scraped_data['Occupation'] += occupations_lst

  def get_companies(self,head,clas):
    self.head= head
    self.clas = clas
    companies = self.soup.find_all(self.head,{'class':self.clas})
    companies_lst = [company.text for company in companies]
    self.scraped_data['Company'] += companies_lst

  def get_specs(self,head,clas):
    self.head= head
    self.clas = clas
    specs = self.soup.find_all(self.head, {'class':self.clas})
    specs_lst = [spec.text for spec in specs]
    self.scraped_data['Specs'] += specs_lst

  def get_location(self,head,clas):
    self.head= head
    self.clas = clas
    locations = self.soup.find_all(self.head,{'class':self.clas})
    locations_lst = [location.text for location in locations]
    self.scraped_data['Location'] += locations_lst

ML = webscrap()
for i in range(3):
  ML.fetch(f'https://wuzzuf.net/search/jobs/?a=hpb&q=machine%20learning&start={i}')

  ML.get_titles('h2','css-m604qf')
  ML.get_links('h2','css-m604qf')
  ML.get_occupations('div','css-1lh32fc')
  ML.get_companies('a','css-17s97q8')
  ML.get_specs('div','css-y4udm8')
  ML.get_location('span','css-5wys0k')

ML_df = pd.DataFrame(ML.scraped_data)

DA = webscrap()
for i in range(53):
  DA.fetch(f'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20analysis&start={i}')

  DA.get_titles('h2','css-m604qf')
  DA.get_links('h2','css-m604qf')
  DA.get_occupations('div','css-1lh32fc')
  DA.get_companies('a','css-17s97q8')
  DA.get_specs('div','css-y4udm8')
  DA.get_location('span','css-5wys0k')

DA_df = pd.DataFrame(DA.scraped_data)

DS = webscrap()
for i in range(31):
  DS.fetch(f'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20science&start={i}')

  DS.get_titles('h2','css-m604qf')
  DS.get_links('h2','css-m604qf')
  DS.get_occupations('div','css-1lh32fc')
  DS.get_companies('a','css-17s97q8')
  DS.get_specs('div','css-y4udm8')
  DS.get_location('span','css-5wys0k')

DS_df = pd.DataFrame(DS.scraped_data)

BI = webscrap()
for i in range(8):
  BI.fetch(f'https://wuzzuf.net/search/jobs/?a=navbg&q=business%20intelligence&start={i}')

  BI.get_titles('h2','css-m604qf')
  BI.get_links('h2','css-m604qf')
  BI.get_occupations('div','css-1lh32fc')
  BI.get_companies('a','css-17s97q8')
  BI.get_specs('div','css-y4udm8')
  BI.get_location('span','css-5wys0k')

BI_df = pd.DataFrame(BI.scraped_data)

ML_df

DA_df

DS_df

BI_df

all_df = pd.concat([ML_df,DA_df,DS_df,BI_df],ignore_index=True)
all_df

all_df.to_csv("ml_jobs.csv",index=False)